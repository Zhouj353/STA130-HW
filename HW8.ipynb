{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899ac198",
   "metadata": {},
   "source": [
    "2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following metrics below: provide your answers and, in your own words, concisely explain your rationale for your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a890b2",
   "metadata": {},
   "source": [
    "1. Accuracy measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "It can use for weather prediction about tomorrow will be a rain day or not because accuracy can reflect the correct performence.\n",
    "\n",
    "2. Sensitivity measures the proportion of actual positives that are correctly identified.\n",
    "It can use for medical diagnostics because there is a significant influence if the test miss some real cases. Sensitivity will become more helpful in this way.\n",
    "\n",
    "3. Specificity measures the proportion of actual negatives that are correctly identified.\n",
    "It can use for spam email filtering because it will not make important emails to spam emails easily. Even specificity might leak some spam eamils.\n",
    "\n",
    "4. Precision measures the proportion of positive identifications that were actually correct.\n",
    "It can use for criminal investigation because it want to find the real suspects but not the people who are not connect with this crime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c94e7",
   "metadata": {},
   "source": [
    "4. Create an 80/20 split with 80% of the data as a training set ab_reduced_noNaN_train and 20% of the data testing set ab_reduced_noNaN_test using either df.sample(...) as done in TUT or using train_test_split(...) as done in the previous HW, and report on how many observations there are in the training data set and the test data set.\n",
    "\n",
    "Tell a ChatBot that you are about to fit a \"scikit-learn\" DecisionTreeClassifier model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree clf using only the List Price variable to predict whether or not a book is a hard cover or paper back book using a max_depth of 2; finally use tree.plot_tree(clf) to explain what predictions are made based on List Price for the fitted clf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dac06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming ab_reduced_noNaN is your DataFrame\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(ab_reduced_noNaN, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reporting the size of each dataset\n",
    "print(\"Training set observations:\", len(ab_reduced_noNaN_train))\n",
    "print(\"Testing set observations:\", len(ab_reduced_noNaN_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58305b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Define the features and target\n",
    "X_train = ab_reduced_noNaN_train[['List Price']]  # Feature: List Price\n",
    "y_train = ab_reduced_noNaN_train['Book Type']  # Target: Hard cover or Paperback\n",
    "\n",
    "# Create and fit the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(clf, feature_names=['List Price'], class_names=clf.classes_, filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6b700",
   "metadata": {},
   "source": [
    "6. Use previously created ab_reduced_noNaN_test to create confusion matrices for clf and clf2. Report the sensitivity, specificity and accuracy for each of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82323dbd",
   "metadata": {},
   "source": [
    "Sensitivity: If you care about identifying correct positive cases, you should use sensitivity.\n",
    "\n",
    "Specificity: If you want to find the minimum of risk of false positives, you should use specificity. \n",
    "\n",
    "Accuracy:  Accyracy gives the overall view of wellness of model doing but you should care about is it imbalance model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca12e96",
   "metadata": {},
   "source": [
    "7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for clf and clf2) are better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789bc77",
   "metadata": {},
   "source": [
    "The first confusion matrix only have 'List Price'. The second confusion matrix have three vectors 'NumPages','Thick','List Price'. The two confusion matrices above are better because they have more data test and they can provide more visually model performence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e74d9",
   "metadata": {},
   "source": [
    "ChatGPT link: https://chatgpt.com/share/673fd401-7f78-800c-b11b-56547e96807f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
